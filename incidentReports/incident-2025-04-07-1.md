# Incident: 2025-04-07 14-10-00

## Summary

```md
Between the times of 14:28 and 14:43 (UTC) on April 7th 2025, all users were suddenly unable to purchase pizzas. The event was triggered by an unknown error at the pizza factory at the time 14:28. The users in this time frame received a "Failed to fulfill order at factory" error message in their UI when attempting to buy any number of pizzas.

This bug was detected by grafana alerts and alerted Jordan Johns via email at 14:29. He started working on the problem at 14:35 and had it resolved by 14:43. 

11 pizzas were unable to be purchased by 11 users. This event was a high-severity level event as 100% of users were affected (as seen in grafana dashboard).
```

## Detection

```md
This incident was detected when the grafana alerts alerted Jordan Johns due to failing requests. Jordan Johns was alerted via email and promptly fixed the issue.
```

## Impact

```md
For 15 minutes between 14:28 UTC and 14:43 UTC on April 7th 2025, 100% of users were unable to order pizzas. Thankfully this happened at a low traffic time and the error was able to be fixed in 15 minutes. However 11 pizzas were unable to be purchased by 11 users resulting in a loss of 0.55 BTC.
```

## Timeline

```md
All times are UTC.

- _14:28_ - Problem began.
- _14:29_ - Grafana Alerts alerted Jordan Johns due to failing requests.
- _14:35_ - Jordan began working on the issue.
- _14:43_ - Problem Resolved.
- _14:45_ - Grafana dashboard confirms problem is resolved (pizzas can be purchased).
```

## Response

```md
After receiving an email at 14:29 UTC, on-call engineer Jordan Johns came online at 14:35. He checked the logs in the Grafana Dashboard and was able to quickly solve the issue by following a pizza factory URL path with a fix code in the query string.
```

## Root cause

> [!NOTE]
> Note the final root cause of the incident, the thing identified that needs to change in order to prevent this class of incident from happening again.

```md
**EXAMPLE**:

A bug in connection pool handling led to leaked connections under failure conditions, combined with lack of visibility into connection state.
```

## Resolution

> [!NOTE]
> Describe how the service was restored and the incident was deemed over. Detail how the service was successfully restored and you knew how what steps you needed to take to recovery.
> Depending on the scenario, consider these questions: How could you improve time to mitigation? How could you have cut that time by half?

```md
**EXAMPLE**:
By Increasing the size of the BuildEng EC3 ASG to increase the number of nodes available to support the workload and reduce the likelihood of scheduling on oversubscribed nodes

Disabled the Escalator autoscaler to prevent the cluster from aggressively scaling-down
Reverting the Build Engineering scheduler to the previous version.
```

## Prevention

> [!NOTE]
> Now that you know the root cause, can you look back and see any other incidents that could have the same root cause? If yes, note what mitigation was attempted in those incidents and ask why this incident occurred again.

```md
**EXAMPLE**:

This same root cause resulted in incidents HOT-13432, HOT-14932 and HOT-19452.
```

## Action items

> [!NOTE]
> Describe the corrective action ordered to prevent this class of incident in the future. Note who is responsible and when they have to complete the work and where that work is being tracked.

```md
**EXAMPLE**:

1. Manual auto-scaling rate limit put in place temporarily to limit failures
1. Unit test and re-introduction of job rate limiting
1. Introduction of a secondary mechanism to collect distributed rate information across cluster to guide scaling effects
```
